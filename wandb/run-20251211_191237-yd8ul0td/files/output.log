[34m[1mwandb[0m: Detected [litellm, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/

[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.



[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.
Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/llms/openai.py", line 365, in completion
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/llms/openai.py", line 324, in completion
    response = openai_client.chat.completions.create(**data, timeout=timeout)  # type: ignore
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'id': 'oNopT9w-2kFHot-9ac69bd75bed12b5', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 882, in completion
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 856, in completion
    response = openai_chat_completions.completion(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/llms/openai.py", line 371, in completion
    raise OpenAIError(status_code=e.status_code, message=str(e))
litellm.llms.openai.OpenAIError: Error code: 400 - {'id': 'oNopT9w-2kFHot-9ac69bd75bed12b5', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2550, in wrapper
    result = original_function(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 1936, in completion
    raise exception_type(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 7855, in exception_type
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 6648, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: Together_aiException - Error code: 400 - {'id': 'oNopT9w-2kFHot-9ac69bd75bed12b5', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/main.py", line 217, in <module>
    main(args)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/main.py", line 40, in main
    extracted_attack_list = attackLM.get_attack(convs_list, processed_response_list)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/conversers.py", line 152, in get_attack
    valid_outputs, new_adv_prompts = self._generate_attack(processed_convs_list, init_message)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/conversers.py", line 106, in _generate_attack
    outputs_list = self.model.batched_generate(convs_subset,
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/language_models.py", line 76, in batched_generate
    outputs = litellm.batch_completion(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 2098, in batch_completion
    results = [future.result() for future in completions]
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 2098, in <listcomp>
    results = [future.result() for future in completions]
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2618, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 1968, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2647, in wrapper
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2550, in wrapper
    result = original_function(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 1936, in completion
    raise exception_type(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 7855, in exception_type
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 6648, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: Together_aiException - Error code: 400 - {'id': 'oNopUMR-2kFHot-9ac69bf1581ff4f6', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}
Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/llms/openai.py", line 365, in completion
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/llms/openai.py", line 324, in completion
    response = openai_client.chat.completions.create(**data, timeout=timeout)  # type: ignore
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'id': 'oNopT9w-2kFHot-9ac69bd75bed12b5', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 882, in completion
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 856, in completion
    response = openai_chat_completions.completion(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/llms/openai.py", line 371, in completion
    raise OpenAIError(status_code=e.status_code, message=str(e))
litellm.llms.openai.OpenAIError: Error code: 400 - {'id': 'oNopT9w-2kFHot-9ac69bd75bed12b5', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2550, in wrapper
    result = original_function(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 1936, in completion
    raise exception_type(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 7855, in exception_type
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 6648, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: Together_aiException - Error code: 400 - {'id': 'oNopT9w-2kFHot-9ac69bd75bed12b5', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/main.py", line 217, in <module>
    main(args)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/main.py", line 40, in main
    extracted_attack_list = attackLM.get_attack(convs_list, processed_response_list)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/conversers.py", line 152, in get_attack
    valid_outputs, new_adv_prompts = self._generate_attack(processed_convs_list, init_message)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/conversers.py", line 106, in _generate_attack
    outputs_list = self.model.batched_generate(convs_subset,
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/language_models.py", line 76, in batched_generate
    outputs = litellm.batch_completion(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 2098, in batch_completion
    results = [future.result() for future in completions]
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 2098, in <listcomp>
    results = [future.result() for future in completions]
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2618, in wrapper
    return litellm.completion_with_retries(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 1968, in completion_with_retries
    return retryer(original_function, *args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/dsi/fetaya-lab/noam_diamant/conda/envs/dict_sae/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2647, in wrapper
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 2550, in wrapper
    result = original_function(*args, **kwargs)
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/main.py", line 1936, in completion
    raise exception_type(
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 7855, in exception_type
    raise e
  File "/private/fetaya-lab/noam_diamant/projects/Unlearning_with_SAE/JailbreakingLLMs/.venv/lib/python3.10/site-packages/litellm/utils.py", line 6648, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: Together_aiException - Error code: 400 - {'id': 'oNopUMR-2kFHot-9ac69bf1581ff4f6', 'error': {'message': 'Unable to access non-serverless model lmsys/vicuna-13b-v1.5. Please visit https://api.together.ai/models/lmsys/vicuna-13b-v1.5 to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}
